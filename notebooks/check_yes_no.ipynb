{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusion\n",
    "from diffusion.models import BertLMHeadModel as OurMaskedAttentionBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000e+00, -3.4028e+38, -0.0000e+00, -0.0000e+00, -3.4028e+38],\n",
       "          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -3.4028e+38],\n",
       "          [-0.0000e+00, -3.4028e+38, -0.0000e+00, -0.0000e+00, -3.4028e+38],\n",
       "          [-0.0000e+00, -3.4028e+38, -0.0000e+00, -0.0000e+00, -3.4028e+38],\n",
       "          [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion.models.modeling_bert import extend_for_label\n",
    "\n",
    "extend_for_label(torch.Tensor([1, 1, 1, 1, 0])[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, -1, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = list(range(10))\n",
    "arr[0:1] = [arr[0], -1]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertLMHeadModel\n",
    "our_bert = OurMaskedAttentionBert.from_pretrained('bert-base-uncased').cuda().eval()\n",
    "h_bert = BertLMHeadModel.from_pretrained('bert-base-uncased').cuda().eval()\n",
    "for param in h_bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in our_bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/tbadmaev/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/tbadmaev/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from diffusion.dataset.glue_data import SST2Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SST2Dataset(train=True)\n",
    "valid_dataset = SST2Dataset(train=False)\n",
    "\n",
    "\n",
    "def get_val_loader():\n",
    "    return DataLoader(valid_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "def get_train_loader():\n",
    "    return DataLoader(train_dataset, batch_size=256, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from diffusion.utils import dict_to_device\n",
    "\n",
    "\n",
    "def print_accuracy(loader, bert_model):\n",
    "    total_true_pred = 0\n",
    "    total_label_pred = 0\n",
    "    total_size = 0\n",
    "    total_word_count = 0\n",
    "\n",
    "    total_label_from_all = 0\n",
    "\n",
    "    bar = tqdm(loader)\n",
    "    for batch in bar:\n",
    "        batch = dict_to_device(batch, 'cuda')\n",
    "        count = torch.sum(batch['attention_mask'], dim=-1).long()\n",
    "        logits = bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
    "        # 0 - [CLS], 1 - label, count-1 - [SEP]\n",
    "        pred_label_from_all = torch.argmax(logits[:, 1], dim=-1)\n",
    "        labels_cat = batch['labels'].view(-1) * 2748 + (1 - batch['labels'].view(-1)) * 2053\n",
    "        total_label_from_all += torch.sum((pred_label_from_all == labels_cat).float())\n",
    "\n",
    "\n",
    "        pred_label_logits = logits[:, 1, [2053, 2748]]\n",
    "        pred_label = torch.argmax(pred_label_logits, dim=-1)\n",
    "        total_label_pred += torch.sum((pred_label == batch['labels'].view(-1)).float())\n",
    "        total_size += len(pred_label)\n",
    "\n",
    "        all_pred_ids = torch.argmax(logits, dim=-1)\n",
    "        correct = (all_pred_ids == batch['input_ids'])\n",
    "        total_word_count += torch.sum(count - 3)\n",
    "        for sent, end_idx in zip(correct, count):\n",
    "            total_true_pred += torch.sum(sent[2:end_idx])\n",
    "\n",
    "        bar.set_description(f'label_acc: {total_label_pred / total_size:.5f}, \\\n",
    "                            text_recon_acc: {total_true_pred / total_word_count:.5f}, \\\n",
    "                            label_acc_from_all: {total_label_from_all/ total_size:.5f}')\n",
    "\n",
    "    return total_label_pred / total_size, total_true_pred / total_word_count, total_label_from_all/ total_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our masked attention bert on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f5562164574d229e5c976e0fdd65eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6250, device='cuda:0'),\n",
       " tensor(0.9393, device='cuda:0'),\n",
       " tensor(0.2317, device='cuda:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_accuracy(get_val_loader(), our_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default bert on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0255848d1f814481b8136daf49b802cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7110, device='cuda:0'),\n",
       " tensor(0.9455, device='cuda:0'),\n",
       " tensor(0.3567, device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_accuracy(get_val_loader(), h_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our masked attention bert on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264f62a7e6e44448865af6a1997ac66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7652, device='cuda:0'),\n",
       " tensor(0.8824, device='cuda:0'),\n",
       " tensor(0.3382, device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_accuracy(get_train_loader(), our_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default bert on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf80f305e2d4ddc8928c2619edf77d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8031, device='cuda:0'),\n",
       " tensor(0.8955, device='cuda:0'),\n",
       " tensor(0.4452, device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_accuracy(get_train_loader(), h_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] no hide new secretions from the parental units [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] no contains no wit, only labored gags [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] yes that loves its characters and communicates something rather beautiful about human nature [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] no remains utterly satisfied to remain the same throughout [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] no on the worst revenge - of - the - nerds cliches the filmmakers could dredge up [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(get_train_loader()))\n",
    "batch = dict_to_device(batch, 'cuda')\n",
    "\n",
    "logits = h_bert(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "pred_label_from_all = torch.argmax(logits[:, 1], dim=-1)\n",
    "labels_cat = batch['labels'].view(-1) * 2748 + (1 - batch['labels'].view(-1)) * 2053\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "input_sent = tokenizer.batch_decode(batch['input_ids'])\n",
    "input_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. no hide new secretions from the parental....... no no no.. no no... no no no new new or or the....... no no no.. no no new.. no no no no no no hide new itemsions from the other....... no',\n",
       " '... no wit, only labored gag....... no wit no no. no.... it no humor any nor and no wit. and. and... no. no. no no all and.. no no - it - much much wit nor only short short....... no',\n",
       " '.. that loves its characters and communicates something rather beautiful about human....... it it it.. it it... it it and communicate, and communicate of. its. and... it it it.. it it yes yes. it it loves its characters and communicate with something and deeply about....',\n",
       " '.. remains utterly satisfied to remain the same....... he no.... no... he he he,, and and,. forever.. forever.. he he he. he he.... he he he he he he,,, and and, forever...... he he',\n",
       " '. no on the worst revenge - of - the - nerds cliches the filmmakers could dr make....... no the the some. - - - the - - punk and movie and the ever could. ever..... no. no no the the the revenge revenge. - the - ne - cr story that the']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sent = tokenizer.batch_decode(pred_ids)\n",
    "pred_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
