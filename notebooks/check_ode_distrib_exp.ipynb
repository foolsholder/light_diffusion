{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394443f2-36bb-4ffd-a384-54f5493c536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 24 15:53:42 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              59W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e58bd7-2094-4620-a504-cb53783c5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['BASE_PATH'] = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770cdf47-2e96-47ba-8f6d-fc5bdf67bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch import FloatTensor, Tensor, LongTensor\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "\n",
    "from lightning import seed_everything, Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.strategies.ddp import DDPStrategy\n",
    "from glob import glob\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion import Config\n",
    "import diffusion\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672bc694-5989-40e8-9e9b-ddc358da4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.dynamics import SDE, RSDE, EulerSolver\n",
    "from diffusion.utils import calc_model_grads_norm, calc_model_weights_norm, filter_losses\n",
    "from diffusion.models.contextual_denoising.modeling_clean_encoder import T5EncoderModel\n",
    "from diffusion.models.contextual_denoising.modeling_noisy_encoder import BertLMHeadModel\n",
    "from diffusion.models.contextual_denoising.score_estimator import ScoreEstimator\n",
    "from diffusion.models.contextual_denoising.typings import EncoderOutput\n",
    "\n",
    "from diffusion.helper import LinearWarmupLR\n",
    "from diffusion.dataset import EncNormalizer, enc_normalizer\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ae91b6-2840-42eb-b043-5574b8f47559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(exp_folder: str, ckpt_num: int, use_ema: bool = True, N: int = 200):\n",
    "    seed_everything(1337, workers=True)\n",
    "\n",
    "    cfg = OmegaConf.load(osp.join(exp_folder, 'config.yaml'))\n",
    "    cfg.lightning_wrapper.sde_cfg.ode_sampling = True\n",
    "    cfg.lightning_wrapper.sde_cfg.N = N\n",
    "    cfg.datamodule.valid_dataloader_cfg.batch_size = 64\n",
    "    yaml_cfg = OmegaConf.to_yaml(cfg)\n",
    "    #print(yaml_cfg)\n",
    "    print(osp.abspath('.'))\n",
    "\n",
    "    wrapped_model = instantiate(cfg.lightning_wrapper, _recursive_=False)\n",
    "    ckpt_path = osp.join(exp_folder, f'step_{ckpt_num}.ckpt')\n",
    "    print(f'ckpt_path={ckpt_path}')\n",
    "    ckpt = torch.load(\n",
    "        ckpt_path,\n",
    "        map_location='cpu'\n",
    "    )\n",
    "    wrapped_model.load_state_dict(\n",
    "        ckpt['state_dict'],\n",
    "        strict=True\n",
    "    )\n",
    "    prefix_folder = 'ema_' if use_ema else ''\n",
    "    if use_ema:\n",
    "        from torch_ema import ExponentialMovingAverage\n",
    "        ema = ExponentialMovingAverage(wrapped_model.parameters(), 0)\n",
    "        ema.load_state_dict(\n",
    "            ckpt['callbacks']['EMACallback']\n",
    "        )\n",
    "        ema.copy_to(wrapped_model.parameters())\n",
    "    #wrapped_model.score_estimator.load_state_dict(\n",
    "    #    torch.load('score_estimator.pth', map_location='cpu')\n",
    "    #)\n",
    "    wrapped_model.eval()\n",
    "    return wrapped_model, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b5ff43-b9cc-4bbb-b50e-d284003e8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tbadmaev/cls_glue_diff/light_diffusion/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['enc_normalizer.enc_mean', 'enc_normalizer.enc_std']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at t5-base were not used when initializing T5EncoderModel: ['decoder.final_layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['enc_normalizer.enc_std', 'enc_normalizer.enc_mean']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESTORED SLAVYAN\n",
      "ckpt_path=../experiments/wiki-pretrain-nam-noisy-067-bs512-t2/step_500000.ckpt\n"
     ]
    }
   ],
   "source": [
    "wrapped_model, cfg = load_model('../experiments/wiki-pretrain-nam-noisy-067-bs512-t2', ckpt_num=500_000, use_ema=True, N=1000)\n",
    "cfg: diffusion.Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d3ed28-e48e-4f7a-a15d-065152546dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"max_steps\": 1000000,\n",
      "    \"seed\": 0,\n",
      "    \"every_n_train_steps\": 50000,\n",
      "    \"grad_clip_norm\": 1.0,\n",
      "    \"project\": \"cross_attention\",\n",
      "    \"exp_name\": \"wiki-pretrain-nam-noisy-067-bs512-t2\",\n",
      "    \"pretrained_path\": null,\n",
      "    \"resume_path\": null,\n",
      "    \"max_epochs\": 50,\n",
      "    \"every_n_epochs\": 1,\n",
      "    \"precision\": \"bf16-mixed\",\n",
      "    \"lightning_wrapper\": {\n",
      "        \"optim_partial\": {\n",
      "            \"_target_\": \"torch.optim.AdamW\",\n",
      "            \"_partial_\": true,\n",
      "            \"lr\": 0.0002,\n",
      "            \"weight_decay\": 0.01,\n",
      "            \"betas\": [\n",
      "                0.9,\n",
      "                0.98\n",
      "            ],\n",
      "            \"eps\": 1e-06\n",
      "        },\n",
      "        \"sched_partial\": {\n",
      "            \"_target_\": \"diffusion.LinearWarmupLR\",\n",
      "            \"_partial_\": true,\n",
      "            \"warmup_steps\": 5000,\n",
      "            \"warmup_start_lr\": 1e-06\n",
      "        },\n",
      "        \"noisy_enc_normalizer_cfg\": {\n",
      "            \"_target_\": \"diffusion.EncNormalizer\",\n",
      "            \"enc_mean_path\": \"wiki_pret_old/encodings-bert_base-wiki-mean.pt\",\n",
      "            \"enc_std_path\": \"wiki_pret_old/encodings-bert_base-wiki-std.pt\"\n",
      "        },\n",
      "        \"clean_enc_normalizer_cfg\": {\n",
      "            \"_target_\": \"diffusion.EncNormalizer\",\n",
      "            \"enc_mean_path\": \"data/t5-base-stats/mean.pth\",\n",
      "            \"enc_std_path\": \"data/t5-base-stats/std.pth\"\n",
      "        },\n",
      "        \"_target_\": \"diffusion.lightning_wrappers.contextual_denoising.SlavaContextualDenoising\",\n",
      "        \"ce_coef\": 0.0,\n",
      "        \"sde_cfg\": {\n",
      "            \"_target_\": \"diffusion.dynamics.CosineSD\",\n",
      "            \"N\": 1000,\n",
      "            \"d\": 10,\n",
      "            \"prediction\": \"x_0\",\n",
      "            \"ode_sampling\": true\n",
      "        }\n",
      "    },\n",
      "    \"datamodule\": {\n",
      "        \"_target_\": \"diffusion.dataset.SimpleDataModule\",\n",
      "        \"_recursive_\": false,\n",
      "        \"train_dataset_cfg\": {\n",
      "            \"_target_\": \"diffusion.dataset.wiki_dataset.WikiDataset\",\n",
      "            \"max_length\": 64,\n",
      "            \"train\": true\n",
      "        },\n",
      "        \"valid_dataset_cfg\": {\n",
      "            \"_target_\": \"diffusion.dataset.wiki_dataset.WikiDataset\",\n",
      "            \"max_length\": 64,\n",
      "            \"train\": false\n",
      "        },\n",
      "        \"train_dataloader_cfg\": {\n",
      "            \"_target_\": \"torch.utils.data.DataLoader\",\n",
      "            \"batch_size\": 512,\n",
      "            \"num_workers\": 16,\n",
      "            \"drop_last\": true,\n",
      "            \"shuffle\": true\n",
      "        },\n",
      "        \"valid_dataloader_cfg\": {\n",
      "            \"_target_\": \"torch.utils.data.DataLoader\",\n",
      "            \"batch_size\": 64,\n",
      "            \"num_workers\": \"${datamodule.train_dataloader_cfg.num_workers}\",\n",
      "            \"drop_last\": false,\n",
      "            \"shuffle\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(OmegaConf.to_container(cfg), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd24c40c-38c4-4dd9-b2f6-98242ef9be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached split indices for dataset at /home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0bcf1810a390dc82.arrow and /home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4395827f851c38ac.arrow\n",
      "/home/tbadmaev/new_conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Found cached dataset parquet (/home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached split indices for dataset at /home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0bcf1810a390dc82.arrow and /home/tbadmaev/.cache/huggingface/datasets/Graphcore___parquet/Graphcore--wikipedia-bert-128-d489528ddee484b2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-4395827f851c38ac.arrow\n"
     ]
    }
   ],
   "source": [
    "datamodule: diffusion.SimpleDataModule = instantiate(cfg.datamodule, _recursive_=False)\n",
    "datamodule.setup()\n",
    "datamodule.valid_dataset.setup_empty_cond(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a406fa20-1e03-4ff6-ac91-0124b531d1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbadmaev/new_conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b417d555ae0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader: DataLoader = datamodule.val_dataloader()[0]\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fac78e-e3c2-491c-ab81-57d2f3eda382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_input_ids', 'clean_attention_mask', 'noisy_input_ids', 'noisy_token_type_ids', 'noisy_attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "iterator_loader = iter(loader)\n",
    "batch = next(iterator_loader)\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63af7443-e7dc-4e04-ae00-5f1154566669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.utils import dict_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514b873c-22bc-4a94-b19b-13339dbfc4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SlavaContextualDenoising(\n",
       "  (noisy_part_encoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (enc_normalizer): EncNormalizer()\n",
       "  )\n",
       "  (clean_part_encoder): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (enc_normalizer): EncNormalizer()\n",
       "  )\n",
       "  (score_estimator): SlavaEstimator(\n",
       "    (time_emb): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (input_blocks): ModuleList(\n",
       "        (0-5): 6 x BertBlock(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output_blocks): ModuleList(\n",
       "        (0-5): 6 x BertBlock(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (time_layers): ModuleList(\n",
       "        (0-11): 12 x Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dict_to_device(batch, device)\n",
    "wrapped_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3241b8c-ef69-434b-b428-f9e6c5d01ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ode_cycle_forward(batch: Dict[str, Tensor]):\n",
    "    wrapped_model.eval()\n",
    "    assert wrapped_model.solver.rsde.ode_sampling is True, \"Not ODE sampling\"\n",
    "    \n",
    "    to_clean_part, to_noise_part = wrapped_model.split_batch(batch)\n",
    "    clean_part: EncoderOutput = wrapped_model.clean_part_encoder.forward(**to_clean_part)\n",
    "    noisy_part: EncoderOutput = wrapped_model.noisy_part_encoder.forward(**to_noise_part)\n",
    "    \n",
    "    noisy_part_attention_mask = torch.ones_like(\n",
    "        batch['noisy_attention_mask']\n",
    "    )\n",
    "    \n",
    "    shape = noisy_part_attention_mask.shape + (clean_part.normed.shape[-1],)\n",
    "    cross_encodings = clean_part.normed\n",
    "    \n",
    "    target_encodings = noisy_part.normed\n",
    "    \n",
    "    cross_attention_mask = to_clean_part['attention_mask']\n",
    "    attn_mask = noisy_part_attention_mask\n",
    "\n",
    "    score_call = partial(\n",
    "        wrapped_model.score_estimator.forward,\n",
    "        cross_attention_mask=cross_attention_mask,\n",
    "        cross_encodings=cross_encodings\n",
    "    )\n",
    "    verbose = True\n",
    "    \n",
    "    prefix_time = 0.001\n",
    "    batch_size = target_encodings.shape[0]\n",
    "    input_t = torch.ones(batch_size, device=device) * prefix_time\n",
    "    marg_forward = wrapped_model.sde.marginal_forward(target_encodings, input_t)\n",
    "    target_encodings = marg_forward['mean']\n",
    "    trajectory = []\n",
    "    scores = []\n",
    "    fst_scores = []\n",
    "    prev_t = 0\n",
    "    wrapped_model.solver.rsde.N = wrapped_model.solver.rsde.sde_obj.N = 1000\n",
    "    with torch.no_grad():\n",
    "        x_t = target_encodings\n",
    "\n",
    "        timesteps = torch.linspace(\n",
    "            0.001,\n",
    "            wrapped_model.sde.T,\n",
    "            wrapped_model.sde.N,\n",
    "            device=device\n",
    "        )\n",
    "        rang = trange if verbose else range\n",
    "        idx = 0\n",
    "        while idx < wrapped_model.sde.N:\n",
    "            old_idx = idx\n",
    "            old_x_t = x_t\n",
    "            fst_drift = None\n",
    "            fst_score = None\n",
    "            for _ in range(10):\n",
    "                t = timesteps[old_idx]\n",
    "                input_t = t * torch.ones(shape[0], device=device)\n",
    "                \n",
    "                cur_true_t = timesteps[idx]\n",
    "                input_t_true = t * torch.ones(shape[0], device=device)\n",
    "\n",
    "                dt = cur_true_t - prev_t\n",
    "                prev_t = cur_true_t\n",
    "\n",
    "                rsde_params = wrapped_model.solver.rsde.sde(score_call, old_x_t, input_t, attn_mask)\n",
    "                score = rsde_params['score']\n",
    "                scores += [score.detach().cpu()]\n",
    "                drift = rsde_params['drift']\n",
    "                \n",
    "                if fst_drift is None:\n",
    "                    fst_drift = drift\n",
    "                    fst_score = score\n",
    "                # return rsde_params\n",
    "                fst_scores += [fst_score.detach().cpu()]\n",
    "\n",
    "                x_t = x_t + fst_drift * dt\n",
    "                trajectory += [x_t.detach().cpu()]\n",
    "                idx += 10\n",
    "                break\n",
    "    return trajectory, target_encodings, scores, fst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68fc0e53-47a7-48b2-aa31-68aa2513ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory, target_encodings, scores, fst_scores = ode_cycle_forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3479ba-4de7-48dc-8a9f-0f7b2c68ba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8b4100c-177d-48b6-a56f-a17772b5bae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64, 64, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = torch.stack(trajectory, dim=0)\n",
    "trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81045c0b-49d0-4ad3-9fe3-47e82956314a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64, 64, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.stack(scores, dim=0)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f680cbe7-1bbd-4494-bb40-13993b4e2b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64, 64, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_scores = torch.stack(fst_scores, dim=0)\n",
    "fst_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3a850c-e840-40f9-bbe2-6844547b6bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((fst_scores - scores)**2), torch.max((fst_scores - scores)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "500796fa-163d-424d-9d34-89a8a84eef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = trajectory[-1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db888554-f01e-4a69-8667-6305fdc69f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.solver.rsde.N = wrapped_model.solver.rsde.sde_obj.N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a34575d2-d62d-4e64-93cc-2c4b58aad0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61e75abfa6347ca998ba4f955e52a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restored_ids, restored_encs = wrapped_model.generate_text(batch, init_x=latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f96952fe-7a96-4154-ac98-56e44aa9797c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0728)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((restored_encs.detach().cpu() - target_encodings.detach().cpu())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "752d91cb-8c0e-43cb-8f9d-1c9145d2a36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0724)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = restored_encs.detach().cpu() - target_encodings.detach().cpu()\n",
    "mask = batch['noisy_attention_mask'].cpu()\n",
    "torch.sum(mask[:, :, None] * diff**2) / torch.sum(mask) / 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35166c74-5c03-4271-bee3-660374608f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.1956e-01, -1.3230e+00,  6.9393e-01, -8.4364e-01, -1.3260e+00],\n",
       "         [ 2.3108e-01,  8.9482e-01,  1.3607e+00,  1.6441e+00, -4.4983e-01],\n",
       "         [-2.7082e-02, -1.2491e+00,  5.0450e-01,  8.3794e-01,  1.4602e+00],\n",
       "         [ 7.3687e-01,  1.7612e+00,  7.2118e-01,  1.1937e+00, -6.5470e-01],\n",
       "         [-9.0234e-01,  8.5327e-04,  4.4019e-01,  7.3879e-01,  2.9459e-01]],\n",
       "\n",
       "        [[-2.0550e-01,  2.5575e-01, -3.3013e-01, -1.3299e+00, -2.0358e+00],\n",
       "         [-5.8279e-01, -6.1960e-01,  7.5877e-01, -7.3819e-01, -8.7371e-01],\n",
       "         [ 1.4553e+00,  1.7218e+00, -1.5250e+00, -7.4379e-01, -2.6390e+00],\n",
       "         [ 3.1970e-01,  8.8302e-01, -1.1145e+00, -5.7619e-01, -2.0392e+00],\n",
       "         [ 5.3649e-01,  1.9490e+00, -3.2450e-01, -4.1016e-01, -1.1201e+00]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restored_encs[:2, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d494fc90-a433-4ec8-9916-bf44c68ccbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2507, -2.0373,  0.9087, -0.6004, -1.7866],\n",
       "         [-0.8276, -1.0747,  1.2896,  1.8512,  0.3422],\n",
       "         [-0.4150, -2.3227,  1.1480,  0.5801,  1.4631],\n",
       "         [-0.6568, -0.7589,  1.1090,  1.9734, -0.0804],\n",
       "         [-1.1263, -1.2149,  0.5550,  0.9605,  0.3802]],\n",
       "\n",
       "        [[-0.1092,  0.2053, -0.4096, -1.3047, -1.9059],\n",
       "         [-0.5177, -0.7863,  0.8896, -0.5846, -0.7339],\n",
       "         [ 1.5270,  2.0378, -1.5762, -0.8646, -2.3814],\n",
       "         [-0.0108,  0.5502, -1.5999, -0.6120, -1.6533],\n",
       "         [ 0.7363,  1.9046, -0.5716, -0.5481, -1.1997]]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encodings[:2, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd5d8cb5-ea4d-450b-b14d-0e6d8ab9232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_100 = torch.load('latent_ode_encs_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d49eb06e-6358-4966-9b70-d55453b1767e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.7016)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((latents_100[:64] - latents.cpu())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "710aa16f-0670-4dd0-b78c-58b9c6bb35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trajectory, 'trajectory_exp_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04d1cff7-944a-48ea-8186-33ce6d1eace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(scores, 'scores_exp_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ec72934-0dda-4f28-b6e8-ab1704f41ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] in 1958, until the opening of the on - campus john f. kennedy memorial pavilion in 1965, later the charlotte y. martin centre. the bulldogs returned to the coliseum in 1979, their first year in the west coast athletic conference, for [SEP] they them they them themm them theytom them [PAD]vity for advise',\n",
       " '[CLS] musician tom taylor, who gave king guitar lessons when king was 12. king\\'s bass playing style is largely based on continuous 16th notes ( aka semiquavers ), sometimes described as \" machine - gun \" style. [SEP] [PAD] [PAD] king he [PAD] [PAD] the is the [PAD] the – [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 16 september 1779 ) was an irish landowner and politician. biography. he was made a freeman of the city of waterford in 1737 and was mayor of waterford from 1743 to 1744 and in 1761. he represented the city in parliament from 1768 to 1776za he was a magistrate for county waterford from 1743 and high sheriff of [SEP]',\n",
       " '[CLS] and dunlop were also asked if they would be interested in the one - make tyre rule contract. partly as a result of the control tyres, motorcycle sports manufacturer association ( aprilia, ducati, honda, kawasaki, suzuki and yamaha ) announced that no ms [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] others. the blockade runners may have been numerous, but they were built for speed rather than the ability to carry cargo. the more conventional cargo vessels, and their spacious holds, went elsewhere die as a result, southern in of cotton fell by 95 % from pre - war levels, parsons was politically influenced by the [SEP]']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = wrapped_model.noisy_part_encoder.classify(normed=restored_encs[:5])\n",
    "text_labels = torch.argmax(logits, dim=-1)\n",
    "loader.dataset.noisy_tokenizer.batch_decode(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca661db6-07ea-4b14-8207-ccdbb08e0688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] in 1958, until the opening of the on - campus john f. kennedy memorial pavilion in 1965, later the charlotte y. martin centre. the bulldogs returned to the coliseum in 1979, their first year in the west coast athletic conference, for [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] musician tom taylor, who gave king guitar lessons when king was 12. king\\'s bass playing style is largely based on continuous 16th notes ( aka semiquavers ), sometimes described as \" machine - gun \" style. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 16 september 1779 ) was an irish landowner and politician. biography. he was made a freeman of the city of waterford in 1737 and was mayor of waterford from 1743 to 1744 and in 1761. he represented the city in parliament from 1768 to 1776. he was a magistrate for county waterford from 1743 and high sheriff of [SEP]',\n",
       " '[CLS] and dunlop were also asked if they would be interested in the one - make tyre rule contract. partly as a result of the control tyres, motorcycle sports manufacturer association ( aprilia, ducati, honda, kawasaki, suzuki and yamaha ) announced that no ms [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] others. the blockade runners may have been numerous, but they were built for speed rather than the ability to carry cargo. the more conventional cargo vessels, and their spacious holds, went elsewhere. as a result, southern exports of cotton fell by 95 % from pre - war levels, parsons was politically influenced by the [SEP]']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = wrapped_model.noisy_part_encoder.classify(normed=target_encodings[:5])\n",
    "text_labels = torch.argmax(logits, dim=-1)\n",
    "loader.dataset.noisy_tokenizer.batch_decode(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b2705d3-9b34-4b26-a8e6-e94bbe598356",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ode_cycle_backward(batch: Dict[str, Tensor], init_state):\n",
    "    wrapped_model.eval()\n",
    "    assert wrapped_model.solver.rsde.ode_sampling is True, \"Not ODE sampling\"\n",
    "    \n",
    "    to_clean_part, to_noise_part = wrapped_model.split_batch(batch)\n",
    "    clean_part: EncoderOutput = wrapped_model.clean_part_encoder.forward(**to_clean_part)\n",
    "    \n",
    "    noisy_part_attention_mask = torch.ones_like(\n",
    "        batch['noisy_attention_mask']\n",
    "    )\n",
    "    \n",
    "    shape = noisy_part_attention_mask.shape + (clean_part.normed.shape[-1],)\n",
    "    cross_encodings = clean_part.normed\n",
    "    \n",
    "    cross_attention_mask = to_clean_part['attention_mask']\n",
    "    attn_mask = noisy_part_attention_mask\n",
    "\n",
    "    score_call = partial(\n",
    "        wrapped_model.score_estimator.forward,\n",
    "        cross_attention_mask=cross_attention_mask,\n",
    "        cross_encodings=cross_encodings\n",
    "    )\n",
    "    verbose = True\n",
    "    \n",
    "    batch_size = shape[0]\n",
    "    trajectory = []\n",
    "    scores = []\n",
    "    fst_scores = []\n",
    "    prev_t = 0\n",
    "    with torch.no_grad():\n",
    "        x_t = init_state\n",
    "\n",
    "        timesteps = torch.linspace(\n",
    "            wrapped_model.sde.T,\n",
    "            wrapped_model.sde.T / wrapped_model.sde.N,\n",
    "            wrapped_model.sde.N,\n",
    "            device=device\n",
    "        )\n",
    "        rang = trange if verbose else range\n",
    "        idx = 0\n",
    "        while idx < wrapped_model.sde.N:\n",
    "            old_idx = idx\n",
    "            old_x_t = x_t\n",
    "            fst_drift = None\n",
    "            fst_score = None\n",
    "            for _ in range(10):\n",
    "                t = timesteps[old_idx]\n",
    "                input_t = t * torch.ones(shape[0], device=device)\n",
    "                \n",
    "                cur_true_t = timesteps[idx]\n",
    "                input_t_true = t * torch.ones(shape[0], device=device)\n",
    "\n",
    "                dt = cur_true_t - prev_t\n",
    "                prev_t = cur_true_t\n",
    "\n",
    "                rsde_params = wrapped_model.solver.rsde.sde(score_call, old_x_t, input_t, attn_mask)\n",
    "                score = rsde_params['score']\n",
    "                scores += [score.detach().cpu()]\n",
    "                drift = rsde_params['drift']\n",
    "                \n",
    "                if fst_drift is None:\n",
    "                    fst_drift = drift\n",
    "                    fst_score = score\n",
    "                # return rsde_params\n",
    "                fst_scores += [fst_score.detach().cpu()]\n",
    "\n",
    "                x_t = x_t + fst_drift * dt\n",
    "                trajectory += [x_t.detach().cpu()]\n",
    "                idx += 1\n",
    "    return trajectory, scores, fst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09332dcc-b4c5-4905-b618-f93674c81667",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_trajectory, b_scores, b_fst_scores = ode_cycle_backward(batch, latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9587ab3e-ee2b-4658-a0ff-8c8a9c49534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4175)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((b_trajectory[-1].detach().cpu() - noisy_part.normed.detach().cpu())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469d093-ff2a-4f17-b92e-59d0cda9d713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [new_conda]",
   "language": "python",
   "name": "conda-env-new_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
